{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron Neural Network  (MLP ANN) - Heuristics on weights hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we would like to test whether discussed heuristic approaches could be applied on MLP for weights hyperparameters estimation without training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function description\n",
    "As an objective function for a heuristic approach we use a loss function of the MLP. We defined 2 loss functions - **Mean Square Error** and **Cross Entropy Loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Square Loss\n",
    "$$C(w,b)=\\frac{1}{2n}\\sum_x||y(x)-a||^2 + \\frac{\\lambda}{2n}\\sum_w||w||^2$$\n",
    "where:\n",
    "* $w$ is a MLP weights tensor\n",
    "* $b$ is a MLP bias tensor\n",
    "* $n$ is a number of data records in training dataset\n",
    "* $x$ is a particular datum record\n",
    "* $y(x)$ is a class membership vector predicted by MLP\n",
    "* $a$ is a ground truth memebership vector\n",
    "* $\\lambda$ is a regularization term coeficient\n",
    "\n",
    "Implementation of the loss function is located in `src/heur_aux.py`, class `MSRLoss`. In does not inherit interface from `ObjFun` because based on the logic of ANN, it makes sence to implement core common MLP primitives into class `ANNMLPClassifier`, located in `src/objfun_ann_mlp.py` and use `MSRLoss` as its attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss\n",
    "$$C(w,b)=-[y(x)\\mathrm{ln}(a)+(1-y(x))\\mathrm{ln}(1-a)] + \\frac{\\lambda}{2n}\\sum_w||w||^2$$\n",
    "where:\n",
    "* $w$ is a MLP weights tensor\n",
    "* $b$ is a MLP bias tensor\n",
    "* $n$ is a number of data records in training dataset\n",
    "* $x$ is a particular datum record\n",
    "* $y(x)$ is a class membership vector predicted by MLP\n",
    "* $a$ is a ground truth memebership vector\n",
    "* $\\lambda$ is a regularization term coeficient\n",
    "\n",
    "Implementation of the loss function is located in `src/heur_aux.py`, class `CrossEntropyLoss`. In does not inherit interface from `ObjFun` because based on the logic of ANN, it makes sence to implement core common MLP primitives into class `ANNMLPClassifier`, located in `src/objfun_ann_mlp.py` and use `CrossEntropyLoss` as its attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More loss functions could be added by creating a new particular class with reguired interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "We use **Sigmoid** activation function (class `SigmoidFunction` in a `src/heur_aux.py`) which has following shape\n",
    "<img src=\"img/sigmoid.png\">\n",
    "More activation functions could be added by creating a new particular class with reguired interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used dataset\n",
    "Our implementation can work with any datasets. It is desired that data matrix has shape ($m$,$n$) where $m$ is records number and $n$ is features count (float). Labels for each class has to be only integers - shape($m$,1), starts with 0 and incremented 1 by 1. Objective function automatically divide datasets into training, testing split and automatically shuffled.\n",
    "\n",
    "In this notebook we use **Iris** dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import path to source directory (bit of a hack in Jupyter)\n",
    "import sys\n",
    "import os\n",
    "pwd = %pwd\n",
    "sys.path.append(os.path.join(pwd, os.path.join('..', 'src')))\n",
    "\n",
    "# Ensure modules are reloaded on any change (very useful when developing code on the fly)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import extrenal librarires\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris_dataset = datasets.load_iris()\n",
    "\n",
    "iris_features = iris_dataset.data[:, :]\n",
    "iris_labels = iris_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(iris_labels).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Training\n",
    "As a first step we need to train MLP to obtain $f^*$. For training it is implemented Stochastic Gradient Descent. Training is performed automatically when constructor of the MLP objective function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our code\n",
    "from objfun_ann_mlp import ANNMLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set MLP parameters. Now we test MLP without any hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_hidden_layers = []\n",
    "number_of_epochs = 2000\n",
    "batch_size_sgd = 20\n",
    "learning_rate = 0.01\n",
    "reg_lambda = 0.01\n",
    "loss_function = \"MSR\"\n",
    "activation_function = \"sigmoid\"\n",
    "features = iris_features\n",
    "labels = iris_labels\n",
    "training_data_size_percentage_split = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* `batch_size_sgd` should be lower, equal to number of **training records**. \n",
    "* `loss_function` can take values `MSR` or `cross-entropy`.\n",
    "* `activation_function` can take values `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'heur_aux' has no attribute 'MSRLoss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-21ac172a9537>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m mlp = ANNMLPClassifier(neurons_hidden_layers, number_of_epochs, batch_size_sgd, \n\u001b[0;32m      2\u001b[0m                          \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                          features, labels, training_data_size_percentage_split)\n\u001b[0m",
      "\u001b[1;32mD:\\Libraries\\Downloads\\heur_src\\heur_src\\classes\\..\\assignments\\ANN_MLP\\src\\objfun_ann_mlp.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, par_numOfNeuronsInHiddenLayers, par_epochsNum, par_batchSize, par_learningRate, par_lambda, par_lossFunction, par_activationFunction, par_data, par_labels, par_dataSplit)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m#TRAIN NETWORK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_batchSize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlossFunction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolveLossFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar_lossFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivationFunction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolveActivationFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar_activationFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxNumberOfEpochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_epochsNum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Libraries\\Downloads\\heur_src\\heur_src\\classes\\..\\assignments\\ANN_MLP\\src\\objfun_ann_mlp.py\u001b[0m in \u001b[0;36mresolveLossFunc\u001b[1;34m(self, lossfuncPar)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresolveLossFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossfuncPar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlossfuncPar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"MSR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mann_heur_aux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSRLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlossfuncPar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cross-entropy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mann_heur_aux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'heur_aux' has no attribute 'MSRLoss'"
     ]
    }
   ],
   "source": [
    "mlp = ANNMLPClassifier(neurons_hidden_layers, number_of_epochs, batch_size_sgd, \n",
    "                         learning_rate, reg_lambda, loss_function, activation_function, \n",
    "                         features, labels, training_data_size_percentage_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print final epoch statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_training': 0.9333333333333333,\n",
       " 'loss_training': 0.1341185767535982,\n",
       " 'precision_testing': 0.9666666666666667,\n",
       " 'loss_testing': 0.033453965834250275,\n",
       " 'epoch': 2000}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.trainingStatusInfo[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this dataset could be handled without any hidden layer, so we could add shallow hidden layers to demo purposes. We add 10 hidden neurons in one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_hidden_layers = [5]\n",
    "number_of_epochs = 2000\n",
    "batch_size_sgd = 20\n",
    "learning_rate = 0.1\n",
    "reg_lambda = 0.01\n",
    "loss_function = \"MSR\"\n",
    "activation_function = \"sigmoid\"\n",
    "features = iris_features\n",
    "labels = iris_labels\n",
    "training_data_size_percentage_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Installations\\Python36-32\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "mlp = ANNMLPClassifier(neurons_hidden_layers, number_of_epochs, batch_size_sgd, \n",
    "                         learning_rate, reg_lambda, loss_function, activation_function, \n",
    "                         features, labels, training_data_size_percentage_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print final epoch statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.trainingStatusInfo[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that one hidden layer increased the precision of the classifier. We can experiment with different shape of hidden layers and MLP training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (MSE loss function) Heuristics\n",
    "\n",
    "Based on the previous training we obtained the $f^*$ which corresponds to the loss for **testing** data in the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04445719492146986"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_fstar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a question whether the domain of weights should be bounded or not. During the training there are no bounds for the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numberOfLayers = len(mlp.weightsTensor)\n",
    "\n",
    "for tensorInd in range(0,numberOfLayers-1):\n",
    "    plt.figure(tensorInd)\n",
    "    plt.title(\"Hidden Layer {}\".format(tensorInd+1))\n",
    "    plt.hist(mlp.weightsTensor[tensorInd].flatten(), bins='auto')\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure(numberOfLayers-1)\n",
    "plt.title(\"Output Layer\")\n",
    "plt.hist(mlp.weightsTensor[tensorInd].flatten(), bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that weights values are bounded and have shape like normal distribution. It is caused by fact that normal distribution was used during weights initialization. \n",
    "\n",
    "We set weights bounds to $\\left[\\lfloor{\\mathrm{min}(w)\\rfloor}, \\lceil{\\mathrm{max}(w)\\rceil}\\right] $ across all weights elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4, 5]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the dimension of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weightsDim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shoot and Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use **Shoot and Go** to perform a heuristics over weights. Because the search is very time consuming, we will use smaller number of runs per parameter value.\n",
    "\n",
    "We take into count the high dimension of the task. Thus, we will increase the number of evaluations per heuristic run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxeval = 5000\n",
    "NUM_RUNS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heur_sg import ShootAndGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_sg(of, maxeval, num_runs, hmax, random_descent):\n",
    "    method = 'RD' if random_descent else 'SD'\n",
    "    results = []\n",
    "    for i in tqdm_notebook(range(num_runs), 'Testing method={}, hmax={}'.format(method, hmax)):\n",
    "        result = ShootAndGo(of, maxeval=maxeval, hmax=hmax, random_descent=random_descent).search() # dict with results of one run\n",
    "        result['run'] = i\n",
    "        result['heur'] = 'SG_{}_{}'.format(method, hmax) # name of the heuristic\n",
    "        result['method'] = method\n",
    "        result['hmax'] = hmax\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results, columns=['heur', 'run', 'method', 'hmax', 'best_x', 'best_y', 'neval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cccf7743e2144569921b266af99c8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=0', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cc7df04b444e7e8c7d20e189bbdc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=1', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b30ea895043ecbe9e327a498a8249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=2', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a530d9adc855412b8deccc1a506d54cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=5', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7c3fab4ae84ce9bd0eab9798317774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=10', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95101b7b0c04401885f23d9ca82a4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=20', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf796ec853438e9848bdf1d55c33db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=50', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08af1db6fe84ec5bbb6493df020ac61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing method=SD, hmax=inf', max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_sg = pd.DataFrame()\n",
    "    \n",
    "for hmax in [0, 1, 2, 5, 10, 20, 50, np.inf]:\n",
    "    res = experiment_sg(of=mlp, maxeval=maxeval, num_runs=NUM_RUNS, hmax=hmax, random_descent=False)\n",
    "    table_sg = pd.concat([table_sg, res], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heur</th>\n",
       "      <th>run</th>\n",
       "      <th>method</th>\n",
       "      <th>hmax</th>\n",
       "      <th>best_x</th>\n",
       "      <th>best_y</th>\n",
       "      <th>neval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>0</td>\n",
       "      <td>SD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3.099414162011981, 2.468834923870892, 2.40509...</td>\n",
       "      <td>0.227567</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>1</td>\n",
       "      <td>SD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8603129452802873, -0.2491965057975296, -1.6...</td>\n",
       "      <td>0.269351</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>2</td>\n",
       "      <td>SD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.7487455153631544, -3.679267122263559, -3.6...</td>\n",
       "      <td>0.280154</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>3</td>\n",
       "      <td>SD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.39926545940294167, 3.9494078476585566, 1.1...</td>\n",
       "      <td>0.265011</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>4</td>\n",
       "      <td>SD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4.739663426525537, 3.955386448163689, 0.68612...</td>\n",
       "      <td>0.233305</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      heur  run method  hmax  \\\n",
       "0  SG_SD_0    0     SD   0.0   \n",
       "1  SG_SD_0    1     SD   0.0   \n",
       "2  SG_SD_0    2     SD   0.0   \n",
       "3  SG_SD_0    3     SD   0.0   \n",
       "4  SG_SD_0    4     SD   0.0   \n",
       "\n",
       "                                              best_x    best_y  neval  \n",
       "0  [3.099414162011981, 2.468834923870892, 2.40509...  0.227567    inf  \n",
       "1  [0.8603129452802873, -0.2491965057975296, -1.6...  0.269351    inf  \n",
       "2  [-0.7487455153631544, -3.679267122263559, -3.6...  0.280154    inf  \n",
       "3  [-0.39926545940294167, 3.9494078476585566, 1.1...  0.265011    inf  \n",
       "4  [4.739663426525537, 3.955386448163689, 0.68612...  0.233305    inf  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_loss(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def std_dev(x):\n",
    "    return np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_sg = table_sg.pivot_table(\n",
    "    index=['heur'],\n",
    "    values=['best_y'],\n",
    "    aggfunc=(mean_loss, std_dev)\n",
    ")['best_y']\n",
    "stats_sg = stats_sg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heur</th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>0.242135</td>\n",
       "      <td>0.026794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SG_SD_1</td>\n",
       "      <td>0.379634</td>\n",
       "      <td>0.052112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SG_SD_10</td>\n",
       "      <td>0.489913</td>\n",
       "      <td>0.096093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SG_SD_2</td>\n",
       "      <td>0.409837</td>\n",
       "      <td>0.066055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SG_SD_20</td>\n",
       "      <td>0.541116</td>\n",
       "      <td>0.115404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SG_SD_5</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.082456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SG_SD_50</td>\n",
       "      <td>0.564590</td>\n",
       "      <td>0.141617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SG_SD_inf</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.174133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        heur  mean_loss   std_dev\n",
       "0    SG_SD_0   0.242135  0.026794\n",
       "1    SG_SD_1   0.379634  0.052112\n",
       "2   SG_SD_10   0.489913  0.096093\n",
       "3    SG_SD_2   0.409837  0.066055\n",
       "4   SG_SD_20   0.541116  0.115404\n",
       "5    SG_SD_5   0.459300  0.082456\n",
       "6   SG_SD_50   0.564590  0.141617\n",
       "7  SG_SD_inf   0.587133  0.174133"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heur</th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SG_SD_0</td>\n",
       "      <td>0.242135</td>\n",
       "      <td>0.026794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SG_SD_1</td>\n",
       "      <td>0.379634</td>\n",
       "      <td>0.052112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SG_SD_2</td>\n",
       "      <td>0.409837</td>\n",
       "      <td>0.066055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SG_SD_5</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.082456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SG_SD_10</td>\n",
       "      <td>0.489913</td>\n",
       "      <td>0.096093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SG_SD_20</td>\n",
       "      <td>0.541116</td>\n",
       "      <td>0.115404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SG_SD_50</td>\n",
       "      <td>0.564590</td>\n",
       "      <td>0.141617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SG_SD_inf</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.174133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        heur  mean_loss   std_dev\n",
       "0    SG_SD_0   0.242135  0.026794\n",
       "1    SG_SD_1   0.379634  0.052112\n",
       "3    SG_SD_2   0.409837  0.066055\n",
       "5    SG_SD_5   0.459300  0.082456\n",
       "2   SG_SD_10   0.489913  0.096093\n",
       "4   SG_SD_20   0.541116  0.115404\n",
       "6   SG_SD_50   0.564590  0.141617\n",
       "7  SG_SD_inf   0.587133  0.174133"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_sg.sort_values(by=['mean_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing performance on classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxeval = 5000\n",
    "heuristicsRes = ShootAndGo(mlp, maxeval=maxeval, hmax=0, random_descent=False).search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SG training data loss: 0.22860158274381392\n"
     ]
    }
   ],
   "source": [
    "print('SG training data loss: {}'.format(heuristicsRes['best_y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare it with loss of SGD after first epoch and in the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data loss (epoch 1): 0.4073104752996769\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data loss (epoch 1): {}'.format(mlp.trainingStatusInfo[0][\"loss_training\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data loss (epoch 2000): 0.04445719492146986\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data loss (epoch {}): {}'.format(len(mlp.trainingStatusInfo), mlp.trainingStatusInfo[-1][\"loss_training\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss value of SG is lower then the loss in the first epochs of SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD training data precision: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('GD training data precision: {}'.format(mlp.getTrainingDataPrecision(heuristicsRes['best_x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data precision: {}'.format(mlp.trainingStatusInfo[-1][\"precision_testing\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD testing data precision: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('GD testing data precision: {}'.format(mlp.getTestingDataPrecision(heuristicsRes['best_x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data precision: {}'.format(mlp.trainingStatusInfo[-1][\"precision_testing\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use **Fast Simulated Annealing** to perform a heuristics over weights with Cauchy Mutation. Because the search is very time consuming, we will use smaller number of runs per parameter value.\n",
    "\n",
    "We take into count the high dimension of the task. Thus, we will increase the number of evaluations per heuristic run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxeval = 5000\n",
    "NUM_RUNS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heur_fsa import FastSimulatedAnnealing\n",
    "from heur_aux import Correction, CauchyMutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_fsa(of, maxeval, num_runs, T0, n0, alpha, r):\n",
    "    results = []\n",
    "    for i in tqdm_notebook(range(num_runs), 'Testing T0={}, n0={}, alpha={}, r={}'.format(T0, n0, alpha, r)):\n",
    "        mut = CauchyMutation(r=r, correction=Correction(of))\n",
    "        result = FastSimulatedAnnealing(of, maxeval=maxeval, T0=T0, n0=n0, alpha=alpha, mutation=mut).search()\n",
    "        result['run'] = i\n",
    "        result['heur'] = 'FSA_{}_{}_{}_{}'.format(T0, n0, alpha, r) # name of the heuristic\n",
    "        result['T0'] = T0\n",
    "        result['n0'] = n0\n",
    "        result['alpha'] = alpha\n",
    "        result['r'] = r\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results, columns=['heur', 'run', 'T0', 'n0', 'alpha', 'r', 'best_x', 'best_y', 'neval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ffea3748f74d33ad754e232753b765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1e-10, n0=1, alpha=2, r=0.5', max=300), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2da41c3a684bc0b412c0b9bc9614fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=0.01, n0=1, alpha=2, r=0.5', max=300), HTML(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8439e5dbc94d54beee82921ac13a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=1, alpha=2, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e53c093d754b218c3c908b31b8159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=inf, n0=1, alpha=2, r=0.5', max=300), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_fsa = pd.DataFrame()\n",
    "\n",
    "for T0 in [1e-10, 1e-2, 1, np.inf]:\n",
    "    res = experiment_fsa(of=mlp, maxeval=maxeval, num_runs=NUM_RUNS, T0=T0, n0=1, alpha=2, r=0.5)\n",
    "    table_fsa = pd.concat([table_fsa, res], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heur</th>\n",
       "      <th>run</th>\n",
       "      <th>T0</th>\n",
       "      <th>n0</th>\n",
       "      <th>alpha</th>\n",
       "      <th>r</th>\n",
       "      <th>best_x</th>\n",
       "      <th>best_y</th>\n",
       "      <th>neval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSA_1e-10_1_2_0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[-3.180770642013753, 1.9786721757466752, 5.0, ...</td>\n",
       "      <td>0.079213</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FSA_1e-10_1_2_0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[2.5726959210431373, -3.195356461507299, -0.28...</td>\n",
       "      <td>0.197392</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FSA_1e-10_1_2_0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[-1.7052357884380647, 3.3467673194298935, 5.0,...</td>\n",
       "      <td>0.085492</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSA_1e-10_1_2_0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[-4.0, -0.16023877615673954, -4.0, 5.0, 2.3203...</td>\n",
       "      <td>0.067023</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FSA_1e-10_1_2_0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[-2.4884352502584473, 3.4148511940947683, 2.46...</td>\n",
       "      <td>0.071233</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                heur  run            T0  n0  alpha    r  \\\n",
       "0  FSA_1e-10_1_2_0.5    0  1.000000e-10   1      2  0.5   \n",
       "1  FSA_1e-10_1_2_0.5    1  1.000000e-10   1      2  0.5   \n",
       "2  FSA_1e-10_1_2_0.5    2  1.000000e-10   1      2  0.5   \n",
       "3  FSA_1e-10_1_2_0.5    3  1.000000e-10   1      2  0.5   \n",
       "4  FSA_1e-10_1_2_0.5    4  1.000000e-10   1      2  0.5   \n",
       "\n",
       "                                              best_x    best_y  neval  \n",
       "0  [-3.180770642013753, 1.9786721757466752, 5.0, ...  0.079213    inf  \n",
       "1  [2.5726959210431373, -3.195356461507299, -0.28...  0.197392    inf  \n",
       "2  [-1.7052357884380647, 3.3467673194298935, 5.0,...  0.085492    inf  \n",
       "3  [-4.0, -0.16023877615673954, -4.0, 5.0, 2.3203...  0.067023    inf  \n",
       "4  [-2.4884352502584473, 3.4148511940947683, 2.46...  0.071233    inf  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_fsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_loss(x):\n",
    "    return np.mean(x)\n",
    "\n",
    "def std_dev(x):\n",
    "    return np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the task is continuous in all parameters, it does not make sense to measure number of evaluations to find $f^*$. Instead we measure the mean loss and standard deviation to check how the loss function cnverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_fsa = table_fsa.pivot_table(\n",
    "    index=['heur'],\n",
    "    values=['best_y'],\n",
    "    aggfunc=(mean_loss, std_dev)\n",
    ")['best_y']\n",
    "stats_fsa = stats_fsa.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heur</th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSA_0.01_1_2_0.5</td>\n",
       "      <td>0.105107</td>\n",
       "      <td>0.038095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FSA_1_1_2_0.5</td>\n",
       "      <td>0.104511</td>\n",
       "      <td>0.036980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FSA_1e-10_1_2_0.5</td>\n",
       "      <td>0.104920</td>\n",
       "      <td>0.039566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSA_inf_1_2_0.5</td>\n",
       "      <td>0.245564</td>\n",
       "      <td>0.031349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                heur  mean_loss   std_dev\n",
       "0   FSA_0.01_1_2_0.5   0.105107  0.038095\n",
       "1      FSA_1_1_2_0.5   0.104511  0.036980\n",
       "2  FSA_1e-10_1_2_0.5   0.104920  0.039566\n",
       "3    FSA_inf_1_2_0.5   0.245564  0.031349"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_fsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the statistics we see that the loss function convergence does not depend on the initial temperature. On the other hand we can see that infinite value is not suitable for the task.\n",
    "\n",
    "Leets investigae the parameters space for $T_0=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "**Can we improve the best configuration ($T_0=1$)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92550b3ded0545458e624c444e1434d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=1, alpha=1, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4229a109ee8491d8c49cb8376ec5140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=2, alpha=1, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff91bc6d5084ca088d1afb943ae3532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=4, alpha=1, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d994eabdaa0e4c7aaa5b53a941f8b56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=1, alpha=2, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64294304ee440c8a1aa58234a53d075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=2, alpha=2, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1a80ecf6e647eba5fc68d73a822942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=4, alpha=2, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a1e0b46c2244729cbee8d5fdc4b794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=1, alpha=4, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfd31f6d2cc4ee9bbf1b4bc2584ee07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=2, alpha=4, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448833d957814feaa18320608b5b4cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing T0=1, n0=4, alpha=4, r=0.5', max=300), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_fsa = pd.DataFrame()\n",
    "NUM_RUNS = 300\n",
    "\n",
    "for alpha in [1, 2, 4]:\n",
    "    for cooling_par in [1, 2, 4]:\n",
    "        res = experiment_fsa(of=mlp, maxeval=maxeval, num_runs=NUM_RUNS, T0=1, n0=cooling_par, alpha=alpha, r=0.5)\n",
    "        table_fsa = pd.concat([table_fsa, res], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the results with the results in SGD in the first epoch and in the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_fsa = table_fsa.pivot_table(\n",
    "    index=['heur'],\n",
    "    values=['best_y'],\n",
    "    aggfunc=(mean_loss, std_dev)\n",
    ")['best_y']\n",
    "stats_fsa = stats_fsa.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heur</th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSA_1_1_1_0.5</td>\n",
       "      <td>0.099034</td>\n",
       "      <td>0.027971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FSA_1_2_2_0.5</td>\n",
       "      <td>0.102869</td>\n",
       "      <td>0.035489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FSA_1_1_2_0.5</td>\n",
       "      <td>0.105458</td>\n",
       "      <td>0.037376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FSA_1_2_4_0.5</td>\n",
       "      <td>0.105676</td>\n",
       "      <td>0.038351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FSA_1_4_2_0.5</td>\n",
       "      <td>0.105996</td>\n",
       "      <td>0.037730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSA_1_2_1_0.5</td>\n",
       "      <td>0.106440</td>\n",
       "      <td>0.029976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FSA_1_1_4_0.5</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.038692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FSA_1_4_4_0.5</td>\n",
       "      <td>0.108849</td>\n",
       "      <td>0.040868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FSA_1_4_1_0.5</td>\n",
       "      <td>0.110801</td>\n",
       "      <td>0.029694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            heur  mean_loss   std_dev\n",
       "0  FSA_1_1_1_0.5   0.099034  0.027971\n",
       "4  FSA_1_2_2_0.5   0.102869  0.035489\n",
       "1  FSA_1_1_2_0.5   0.105458  0.037376\n",
       "5  FSA_1_2_4_0.5   0.105676  0.038351\n",
       "7  FSA_1_4_2_0.5   0.105996  0.037730\n",
       "3  FSA_1_2_1_0.5   0.106440  0.029976\n",
       "2  FSA_1_1_4_0.5   0.108140  0.038692\n",
       "8  FSA_1_4_4_0.5   0.108849  0.040868\n",
       "6  FSA_1_4_1_0.5   0.110801  0.029694"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_fsa.sort_values(by=['mean_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results it seems that results are quite same. Thus we decide to use $\\alpha=2$ and $n_0=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing performance on classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxeval = 5000\n",
    "heuristicsRes = FastSimulatedAnnealing(mlp, maxeval=maxeval, T0=1, n0=2, alpha=2, mutation=CauchyMutation(r=0.5, correction=Correction(mlp))).search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSA training data loss: 0.10953871241358984\n"
     ]
    }
   ],
   "source": [
    "print('FSA training data loss: {}'.format(heuristicsRes['best_y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare it with loss of SGD after first epoch and in the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data loss (epoch 1): 0.4073104752996769\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data loss (epoch 1): {}'.format(mlp.trainingStatusInfo[0][\"loss_training\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data loss (epoch 2000): 0.04445719492146986\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data loss (epoch {}): {}'.format(len(mlp.trainingStatusInfo), mlp.trainingStatusInfo[-1][\"loss_training\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss value of FSA is lower then the loss in the first epochs of SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSA training data precision: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('FSA training data precision: {}'.format(mlp.getTrainingDataPrecision(heuristicsRes['best_x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data precision: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data precision: {}'.format(mlp.trainingStatusInfo[-1][\"precision_training\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSA testing data precision: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('FSA testing data precision: {}'.format(mlp.getTestingDataPrecision(heuristicsRes['best_x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD testing data precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('SGD testing data precision: {}'.format(mlp.trainingStatusInfo[-1][\"precision_testing\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we achieve quite similar performance as for SGD approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy loss function Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested FSA on MSE loss function. Lets try the Cross Entropy function to check whether the algorithm performs in similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_hidden_layers = [5]\n",
    "number_of_epochs = 2000\n",
    "batch_size_sgd = 20\n",
    "learning_rate = 0.1\n",
    "reg_lambda = 0.01\n",
    "loss_function = \"cross-entropy\"\n",
    "activation_function = \"sigmoid\"\n",
    "features = iris_features\n",
    "labels = iris_labels\n",
    "training_data_size_percentage_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Installations\\Python36-32\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "mlp = ANNMLPClassifier(neurons_hidden_layers, number_of_epochs, batch_size_sgd, \n",
    "                         learning_rate, reg_lambda, loss_function, activation_function, \n",
    "                         features, labels, training_data_size_percentage_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristicsRes = FastSimulatedAnnealing(mlp, maxeval=maxeval, T0=1, n0=2, alpha=2, mutation=CauchyMutation(r=0.5, correction=Correction(mlp))).search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSA training data loss: -1.780022665678004\n"
     ]
    }
   ],
   "source": [
    "print('FSA training data loss: {}'.format(heuristicsRes['best_y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare it with loss of SGD after first epoch and in the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data loss (epoch 1): -0.3020741555001546\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data loss (epoch 1): {}'.format(mlp.trainingStatusInfo[0][\"loss_training\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data loss (epoch 2000): -1.8127412937003557\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data loss (epoch {}): {}'.format(len(mlp.trainingStatusInfo), mlp.trainingStatusInfo[-1][\"loss_training\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss value of FSA is lower then the loss in the first epochs of SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSA training data precision: 0.975\n"
     ]
    }
   ],
   "source": [
    "print('FSA training data precision: {}'.format(mlp.getTrainingDataPrecision(heuristicsRes['best_x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data precision: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data precision: {}'.format(mlp.trainingStatusInfo[-1][\"precision_training\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSA testing data precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('FSA testing data precision: {}'.format(mlp.getTestingDataPrecision(heuristicsRes['best_x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD training data precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('SGD training data precision: {}'.format(mlp.trainingStatusInfo[-1][\"precision_testing\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
